{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datetime import datetime, timedelta\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "\n",
    "# Initialize the recognizer and the Whisper model\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load Whisper model\n",
    "whisper_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise...\n",
      "Listening for speech...\n",
      "Voice detected, starting recording...\n",
      "Silence detected, stopping recording...\n",
      "Audio recorded and saved as recording.wav\n",
      "Transcribing audio with Whisper...\n",
      "Transcription:  Hello!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import whisper\n",
    "import torch\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "from vad import EnergyVAD\n",
    "\n",
    "# Initialize SpeechRecognition and Microphone\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Initialize EnergyVAD\n",
    "vad = EnergyVAD(\n",
    "    sample_rate=16000,\n",
    "    frame_length=30,  # in milliseconds\n",
    "    frame_shift=30,   # in milliseconds\n",
    "    energy_threshold=0.5,  # you may need to adjust this value\n",
    "    pre_emphasis=0.95  # default values\n",
    ")\n",
    "\n",
    "def record_and_transcribe():\n",
    "    with sr.Microphone(sample_rate=16000) as source:\n",
    "        print(\"Adjusting for ambient noise...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        \n",
    "        print(\"Listening for speech...\")\n",
    "\n",
    "        recording = False\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            audio = recognizer.listen(source)\n",
    "            audio_np = np.frombuffer(audio.get_raw_data(), dtype=np.int16).astype(np.float32) / 32768.0\n",
    "            voice_activity = vad(audio_np)\n",
    "\n",
    "            # Check if any frame is detected as speech\n",
    "            if any(voice_activity):\n",
    "                if not recording:\n",
    "                    print(\"Voice detected, starting recording...\")\n",
    "                    recording = True\n",
    "\n",
    "                frames.append(audio.get_raw_data())\n",
    "            else:\n",
    "                if recording:\n",
    "                    print(\"Silence detected, stopping recording...\")\n",
    "                    recording = False\n",
    "                    break\n",
    "        audio_filename=\"recording.wav\"\n",
    "\n",
    "        # Save the recorded audio to a file\n",
    "        with wave.open(audio_filename, 'wb') as wf:\n",
    "            wf.setnchannels(1)  # mono audio\n",
    "            wf.setsampwidth(2)  # sample width in bytes (2 bytes for int16)\n",
    "            wf.setframerate(16000)  # frame rate (sample rate)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "        print(f\"Audio recorded and saved as {audio_filename}\")\n",
    "        \n",
    "        # Transcribe the recorded audio\n",
    "        print(\"Transcribing audio with Whisper...\")\n",
    "        result = whisper_pipeline(audio_filename)\n",
    "        print(\"Transcription:\", result['text'])\n",
    "        return result['text']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query=record_and_transcribe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What can I do for you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Set your API key directly\n",
    "api_key = \"AIzaSyBA2rSQh0CNpWq5s_AOjoPzFXqxlLWv67E\"\n",
    "\n",
    "# Configure the generative AI with the API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Define the model and the query\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "query = query\n",
    "\n",
    "# Generate content using the model\n",
    "response = model.generate_content(query, generation_config=genai.types.GenerationConfig(\n",
    "    stop_sequences=[\".\"],\n",
    "    max_output_tokens=60,\n",
    "    temperature=0.6,\n",
    "))\n",
    "\n",
    "# Print the generated response\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import edge_tts\n",
    "\n",
    "# Function to synthesize speech with Edge TTS\n",
    "async def synthesize_speech(text, voice=\"en-US-AriaNeural\", rate=\"+0%\", pitch=\"+0Hz\", output_file=\"output.wav\"):\n",
    "    communicate = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)\n",
    "    with open(output_file, 'wb') as file:\n",
    "        async for chunk in communicate.stream():\n",
    "            if chunk[\"type\"] == \"audio\":\n",
    "                file.write(chunk[\"data\"])\n",
    "\n",
    "# Text input\n",
    "text = response.text\n",
    "\n",
    "# Tunable parameters\n",
    "print()\n",
    "voice = \"en-US-AriaNeural\" if input(\"Select Voice: \\n 1. Female \\n 2. Male\")==1 else \"en-US-GuyNeural\" # Example: en-US-AriaNeural (Female), en-US-GuyNeural (Male)\n",
    "rate = str(input(\"Enter Rate + for Higher - For lower\")+\"%\")  # Speech speed (e.g., \"-20%\" for slower, \"+20%\" for faster)\n",
    "pitch = str(input(\"Enter Pitch + for Higher - For lower\")+\"Hz\")  # Pitch (e.g., \"+100Hz\" for higher, \"-100Hz\" for lower)\n",
    "output_file = \"output.wav\"\n",
    "\n",
    "await synthesize_speech(text, voice, rate, pitch, output_file)\n",
    "\n",
    "print(f\"Speech synthesized and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
